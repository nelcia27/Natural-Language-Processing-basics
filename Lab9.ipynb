{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Lab9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctHOdeSNWoXo",
        "colab_type": "text"
      },
      "source": [
        "# Named Entity Recognition - Rozpoznawanie encji nazwanych / tagowanie sekwencji\n",
        "\n",
        "Dotychczas na większości zajęć rozważaliśmy problem klasyfikacji, w którym całym dokumentom przypisywalśmy pojedynczą etykietę (sentyment związany z dokumentem, informacja o tym, czy tekst jest spamowy, etykieta mówiąca o tym w jakim języku napisany jest dokument). Warto jednak również wspomnieć o tzw. tagowaniu sekwencji, które dla każdego elementu sekwencji (słowa) nadaje odpowiednią etykietę.\n",
        "\n",
        "Gdzie taka procedura ma zastosowanie? Wymieńmy kilka przykładów \n",
        "<ol>\n",
        "    <li>Wykrywanie wyrażeń dotyczących miejsc, ludzi, czasu, lokalizacji itp. - każde kolejne słowo tagowane jest informacją mówiącą o tym, czy dane słowo jest częścią pożądanego przez nas typu (np. częścią lokalizacji), czy nie (np. z użyciem kodowania IOB, o którym mówiliśmy przy okazji zajęć dotyczących CONLL)</li>\n",
        "    <li>Tagowanie częściami mowy - każde słowo otrzymuje etykietę mówiącą o tym jaka część mowy reprezentowana jest przez aktualny token.</li>\n",
        "    <li>Wykrywanie ważnych z naszego punktu widzenia fraz (nazwy produktów, technologii itp.)</li>\n",
        "    <li>...</li>\n",
        "</ol>\n",
        "\n",
        "Mówiąc o encjach nazwanych (Named Entities) - mówimy o frazach, którym nadaliśmy określony typ, np: \"01.06.2018\" - typ \"Data\", \"Poznań, Polska\" - typ \"Lokalizacja\", \"GeForce 1080 GTX Ultra\" - typ \"Sprzęt Komputerowy\".\n",
        "\n",
        "Dzisiejsze laboratoria dotyczyć będą właśnie tagowania sekwencji słów z użyciem tzw. NERa (Named Entity Recognizer - Detektor encji nazwanych)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i25895HVWoXq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "8c39367c-e793-4731-c302-86be41790f1d"
      },
      "source": [
        "!pip install python-crfsuite"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-crfsuite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\r\u001b[K     |▍                               | 10kB 17.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81kB 3.4MB/s eta 0:00:01\r\u001b[K     |████                            | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 286kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 296kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 307kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 317kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 337kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 348kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 358kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 368kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 378kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 389kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 399kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 409kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 419kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 430kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 440kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 450kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 460kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 471kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 481kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 491kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 501kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 512kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 522kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 532kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 542kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 552kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 563kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 573kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 583kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 593kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 604kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 614kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 624kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 634kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 645kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 655kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 665kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 675kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 686kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 696kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 706kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 716kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 727kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 737kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 747kB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: python-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZLYBxdhW0yR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "2795c1df-b0bb-4848-9d64-14aa14c3574b"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh02d-ZIWoX0",
        "colab_type": "text"
      },
      "source": [
        "## Gotowy NER - SpaCy\n",
        "\n",
        "W przypadku, kiedy chcemy dla tekstów w języku angielskim w szybki sposób wyszukać bardzo popularne frazy typu: data lub lokalizacja - jest bardzo duża szansa, że możemy wykorzystać gotowe modele narzędzi takich jak NLTK, czy SpaCy.\n",
        "\n",
        "**Zadanie1 (1 punkt)**: \n",
        "<ol>\n",
        "<li>Korzystając z dokumentacji SpaCy dotyczącej NERa (https://spacy.io/usage/linguistic-features#section-named-entities) Wyświetl encje nazwane, znalezione w tekście ze zmiennej 'text'. Wykorzystaj model o nazwie 'en_core_web_md'.</li>\n",
        "<li>Sprawdź, jakiego typu encje wyszukiwane są przez NER ze SpaCy w standardowych modelach dla języka angielskiego (https://spacy.io/api/annotation)</li>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-mU6wj6WoX1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "d4bd1431-6f8c-4453-ec6d-fc5b886aa0d1"
      },
      "source": [
        "import spacy\n",
        "\n",
        "text = \"Jim Gates bought 300 shares of Acme Corp. in 2006.\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "for elem in doc.ents:\n",
        "  print(elem.text, elem.label_)\n",
        "\n",
        "# Typy Encji: PERSON,NORP,FAC,ORG,GPE,LOC,PRODUCT,EVENT,WORK_OF_ART,LAW,LANGUAGE,DATE,TIME,PERCENT,MONEY,QUANTITY,ORDINAL,CARDINAL "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jim Gates PERSON\n",
            "300 CARDINAL\n",
            "Acme Corp. ORG\n",
            "2006 DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2-XbsroWoX8",
        "colab_type": "text"
      },
      "source": [
        "Oczekiwany rezultat:\n",
        "<pre>\n",
        "Jim Gates PERSON\n",
        "300 CARDINAL\n",
        "Acme Corp. ORG\n",
        "2006 DATE\n",
        "</pre>\n",
        "\n",
        "## Gotowy NER - NLTK\n",
        "\n",
        "Również NLTK udostępnia nam wykrywanie encji nazwanych. Spróbujmy to zrobić.\n",
        "\n",
        "**Zadanie2 (1 punkt):**\n",
        "Korzystając z dokumentacji NLTK wykonaj wykrywanie encji nazwanych z przykładowego zdania wykonując sekwencję kroków:\n",
        "<ol>\n",
        "    <li>Wykorzystaj funkcję word_tokenize do podziału zdania na tokeny.</li>\n",
        "    <li>Na reprezentacji zwróconej z kroku: \"podział na tokeny\", wykonaj POS-tagging (nadaj każdemu tokenowi część mowy) z użyciem funkcji pos_tag()</li>\n",
        "    <li>Na wyniku tagowania częściami mowy - wykonaj funkcję ne_chunk() do wykrycia encji.</li>\n",
        "    <li>Jeśli wyświetlisz wynik funkcji ne_chunk zobaczysz coś co bardzo luźno przypomina drzewo, użyj funkcji tree2conlltags() aby zamienić tę reprezentację, na trójki CONLL i wyświetl wynik tej funkcji jako rozwiązanie zadania</li>\n",
        "</ol>\n",
        "\n",
        "Funkcje, których należy użyć to word_tokenize, pos_tag, ne_chunk, tree2conlltags. Wszystkie zostały już zaimportowane."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KYTXD954WoX9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "outputId": "37c60454-9dfd-462b-bf66-e4638c9c9efa"
      },
      "source": [
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.chunk import tree2conlltags\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "nltk.download('conll2002')\n",
        "\n",
        "sentence = \"Jim Gates bought 300 shares of Acme Corp. in 2006.\"\n",
        "tokens=word_tokenize(sentence)\n",
        "tags=pos_tag(tokens)\n",
        "after_tree2conlltags = tree2conlltags(ne_chunk(tags))\n",
        "print(after_tree2conlltags)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2002.zip.\n",
            "[('Jim', 'NNP', 'B-PERSON'), ('Gates', 'NNP', 'B-PERSON'), ('bought', 'VBD', 'O'), ('300', 'CD', 'O'), ('shares', 'NNS', 'O'), ('of', 'IN', 'O'), ('Acme', 'NNP', 'B-ORGANIZATION'), ('Corp.', 'NNP', 'I-ORGANIZATION'), ('in', 'IN', 'O'), ('2006', 'CD', 'O'), ('.', '.', 'O')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxzbBq3hWoYE",
        "colab_type": "text"
      },
      "source": [
        "Oczekiwany rezultat:\n",
        "\n",
        "<pre>\n",
        "[('Jim', 'NNP', 'B-PERSON'), ('Gates', 'NNP', 'B-PERSON'), ('bought', 'VBD', 'O'), ('300', 'CD', 'O'), ('shares', 'NNS', 'O'), ('of', 'IN', 'O'), ('Acme', 'NNP', 'B-ORGANIZATION'), ('Corp.', 'NNP', 'I-ORGANIZATION'), ('in', 'IN', 'O'), ('2006', 'CD', 'O'), ('.', '.', 'O')]\n",
        "</pre>\n",
        "## Własny NER - trening z użyciem algorytmu CRF (Conditional Random Fields)\n",
        "\n",
        "Wykrywacze encji wytrenowane są do odnajdywania popularnych typów fraz (Daty, Lokalizacje, Osoby, ...). Co jednak, kiedy chcielibyśmy wykrywać zdefiniowane przez nas typy danych (np. sprzęt komputerowy), które nie są domyśnie wspierane przez istniejące modele? Musielibyśmy wytrenować własnego NERa. Użyjmy paczki 'pycrfsuite' do tego celu.\n",
        "\n",
        "PyCRFSuite implementuje algorytm CRF - bardzo wydajny algorytm, który potrafi uczyć się tagowania poszczególnych słów z użyciem np. kodowania IOB. Aby rozróżnić różne rodzaje encji, często tagi \"I\" i \"B\" kodowania IOB opatruje się dodatkowym sufiksem. Np. B-Date - oznacza początek daty, a I-Location - kontynuację frazy zawierającej lokację.\n",
        "\n",
        "Ponieważ to czy dane słowo jest encją nazwaną zależy zarówno od tego jak dane słowo wygląda, jak i od słów poprzedzających i następujących po aktualnym - w opisie cech CRFów również uwzględnia się informacje o okalających słowach.\n",
        "\n",
        "**Zadanie (2 punkty)** Wytrenuj model, który będzie tagował poszczególne słowa w tekście z użyciem pycfrsuite. Aby to zrobić, wykonaj podzadania w krokach poniżej.\n",
        "\n",
        "Nasz NER będzie się uczyć etykiet na zbiorze tekstów hiszpańskich, które poddane są podziałowi na zdania, tokenizacji, tagowaniem częściami mowy i etykietami encji do wykrycia w formacie IOB. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t215X1OkWoYG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "2a769d1b-b436-4e8f-e5f3-98bf110fa5d2"
      },
      "source": [
        "import nltk\n",
        "import sklearn\n",
        "import pycrfsuite\n",
        "\n",
        "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train')) # załaduj korpus treningowy dla języka hiszpańskiego\n",
        "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))  # załaduj korpus testowy dla języka hiszpańskiego\n",
        "train_sents[2] # wyświetla przykładowe zdanie, aby zobaczyć jak reprezentowane są dane"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('El', 'DA', 'O'),\n",
              " ('Abogado', 'NC', 'B-PER'),\n",
              " ('General', 'AQ', 'I-PER'),\n",
              " ('del', 'SP', 'I-PER'),\n",
              " ('Estado', 'NC', 'I-PER'),\n",
              " (',', 'Fc', 'O'),\n",
              " ('Daryl', 'VMI', 'B-PER'),\n",
              " ('Williams', 'NC', 'I-PER'),\n",
              " (',', 'Fc', 'O'),\n",
              " ('subrayó', 'VMI', 'O'),\n",
              " ('hoy', 'RG', 'O'),\n",
              " ('la', 'DA', 'O'),\n",
              " ('necesidad', 'NC', 'O'),\n",
              " ('de', 'SP', 'O'),\n",
              " ('tomar', 'VMN', 'O'),\n",
              " ('medidas', 'NC', 'O'),\n",
              " ('para', 'SP', 'O'),\n",
              " ('proteger', 'VMN', 'O'),\n",
              " ('al', 'SP', 'O'),\n",
              " ('sistema', 'NC', 'O'),\n",
              " ('judicial', 'AQ', 'O'),\n",
              " ('australiano', 'AQ', 'O'),\n",
              " ('frente', 'RG', 'O'),\n",
              " ('a', 'SP', 'O'),\n",
              " ('una', 'DI', 'O'),\n",
              " ('página', 'NC', 'O'),\n",
              " ('de', 'SP', 'O'),\n",
              " ('internet', 'NC', 'O'),\n",
              " ('que', 'PR', 'O'),\n",
              " ('imposibilita', 'VMI', 'O'),\n",
              " ('el', 'DA', 'O'),\n",
              " ('cumplimiento', 'NC', 'O'),\n",
              " ('de', 'SP', 'O'),\n",
              " ('los', 'DA', 'O'),\n",
              " ('principios', 'NC', 'O'),\n",
              " ('básicos', 'AQ', 'O'),\n",
              " ('de', 'SP', 'O'),\n",
              " ('la', 'DA', 'O'),\n",
              " ('Ley', 'NC', 'B-MISC'),\n",
              " ('.', 'Fp', 'O')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7Erb_50WoYN",
        "colab_type": "text"
      },
      "source": [
        "**Zadanie 2a (1 punkt)** Tworzenie cech. PyCRFSuite oczekuje, że każde słowo opisane będzie zestawem odpowiednich cech w formie pythonowego słownika. Uzupełnij kod funkcji word2features (sekcje TODO) tak, aby stworzyć odpowiednie cechy zgodnie z nazwami i komentarzami do poszczególnych pól."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-lxDokcWoYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2features(sent, i):\n",
        "    word = sent[i][0]  # sent[i] ma postać np. ('Ley', 'NC', 'B-MISC'); Indeks 0 oznacza pierwszy element z nawiasów (tupli), czyli w tym przypadku 'Ley'\n",
        "    postag = sent[i][1] # sent[i] ma postać np. ('Ley', 'NC', 'B-MISC'); Indeks 0 oznacza pierwszy element z nawiasów (tupli), czyli w tym przypadku 'NC'\n",
        "    \n",
        "    features = {      # cechy aktualnego słowo\n",
        "        'bias': 1.0,\n",
        "        'lowercase_word': word.lower(), # TODO, tutaj słowo małymi literami\n",
        "        'word_last_3_chars': word[-3:], # TODO, tutaj ostatnie 3 znaki słowa\n",
        "        'word_last_2_chars': word[-2:], # TODO, tutaj ostatnie 2 znaki słowa\n",
        "        'word_is_uppercase': word.isupper(), # TODO, tutaj flaga (True/False), czy słowo jest uppercase\n",
        "        'word_is_digit': word.isdigit(), # TODO, tutaj flaga (True/False), czy słowo jest liczbą\n",
        "        'postag': postag, # TODO, tutaj pos-tag (patrz początek definicji funkcji)\n",
        "        'postag_first_two_chars': postag[0:2] # TODO, tutaj pierwsze 2 znaki pos-tagu  \n",
        "    }\n",
        "    if i > 0:         # jeśli nasze słowo nie jest pierwszym w zdaniu - dodajmy do zbioru naszych cech cechy poprzedniego tokenu\n",
        "        word1 = sent[i-1][0]    # poprzednie słowo\n",
        "        postag1 = sent[i-1][1]  # poprzedni pos-tag\n",
        "        \n",
        "        features.update({       # funkcja update() na słowniku dopisuje dodatkowe atrybuty do istniejącego słownika\n",
        "            'previous_word_lower': word1.lower(), # TODO, tutaj poprzednie słowo małymi literami\n",
        "            'previous_word_is_upppercase':word1.isupper(), # TODO, tutaj flaga (True/False), czy słowo jest uppercase\n",
        "            'previous_word_postag': postag1, # TODO, tutaj pos-tag poprzedniego słowa \n",
        "            'previous_word_postag_first_two_chars':postag1[0:2] # TODO, tutaj pierwsze 2 znaki pos-tagu  poprzedniego słowa\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True   # jeśli to pierwszy token - ustawmy cechę BOS (Begin of Sentence) na True\n",
        "        \n",
        "    if i < len(sent)-1:          # Jeśli nasze słowo nie jest ostatnim - dodajmy do zbioru cech cechy następnego słowa \n",
        "        word1 = sent[i+1][0]     # następne słowo\n",
        "        postag1 = sent[i+1][1]   # następny postag\n",
        "        \n",
        "        features.update({        # funkcja update() na słowniku dopisuje dodatkowe atrybuty do istniejącego słownika\n",
        "            'next_word_is_lower': word1.islower(),# TODO, tutaj flaga - czy następne słowo małymi literami\n",
        "            'next_word_is_upppercase': word1.isupper(),# TODO, tutaj flaga (True/False), czy słowo jest uppercase\n",
        "            'next_word_postag':postag1, # TODO, tutaj pos-tag następnego słowa \n",
        "            'next_word_postag_first_two_chars': postag1[0:2]# TODO, tutaj pierwsze 2 znaki pos-tagu  następnego słowa\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True   # jeśli to ostatni token - ustawmy cechę EOS (End of Sentence) na True\n",
        "                \n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))] # zamień każde słowo ze zdania na słownik cech"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt9xEByrWoYU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "17178360-444c-46e3-f0fc-8b96c3122f88"
      },
      "source": [
        "sent2features(train_sents[0])[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BOS': True,\n",
              " 'bias': 1.0,\n",
              " 'lowercase_word': 'melbourne',\n",
              " 'next_word_is_lower': False,\n",
              " 'next_word_is_upppercase': False,\n",
              " 'next_word_postag': 'Fpa',\n",
              " 'next_word_postag_first_two_chars': 'Fp',\n",
              " 'postag': 'NP',\n",
              " 'postag_first_two_chars': 'NP',\n",
              " 'word_is_digit': False,\n",
              " 'word_is_uppercase': False,\n",
              " 'word_last_2_chars': 'ne',\n",
              " 'word_last_3_chars': 'rne'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBNtIsduWoYb",
        "colab_type": "text"
      },
      "source": [
        "Oczekiwany rezultat: \n",
        "<pre>\n",
        "{'BOS': True,\n",
        " 'bias': 1.0,\n",
        " 'lowercase_word': 'melbourne',\n",
        " 'next_word_lower': 'False',\n",
        " 'next_word_is_upppercase': False,\n",
        " 'next_word_postag': 'Fpa',\n",
        " 'next_word_postag_first_two_chars': 'Fp',\n",
        " 'postag': 'NP',\n",
        " 'postag_first_two_chars': 'NP',\n",
        " 'word_is_digit': False,\n",
        " 'word_is_uppercase': False,\n",
        " 'word_last_2_chars': 'ne',\n",
        " 'word_last_3_chars': 'rne'}\n",
        "</pre>\n",
        " \n",
        " **Zadanie 2b (1 punkt) - napisz ciała funkcji pomocniczych, które dla aktualnego zdania z train_sents i test_sents zwrócą:**\n",
        " <ul>\n",
        "     <li>sent2labels - zwróci ciąg oczkiwanych etykiet dla każdego wyrazu. parametr sent jest listą słów, z których każde słowo opisane jest trójką: tekst słowa, pos-tag słowa, etykieta słowa; np. ('Abogado', 'NC', 'B-PER') </li>\n",
        "     <li>sent2tokens - analogicznie do powyższego, jednak zamiast etykiet zwróci ciąg słów bez pos-tagów i etykiet.</li>\n",
        "     <li>get_all_labels - funkcja, która ze zbioru wszystkich zdań treningowych wyświetli zbiór etykiet (zbiór, czyli bez powtórzeń). Funkcja pokaże nam ilu etykiet chcemy się nauczyć, aby móc ocenić trudność naszego problemu.</li>\n",
        " </ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xjDgakgWoYd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "cbd1a273-cdf3-4042-e3c5-e997fa05e0f7"
      },
      "source": [
        "def sent2labels(sent):\n",
        "  res=[]\n",
        "  for s in sent:\n",
        "    res.append(s[2])\n",
        "  return res\n",
        "\n",
        "def sent2tokens(sent):\n",
        "  res=[]\n",
        "  for s in sent:\n",
        "    res.append(s[0])\n",
        "  return res\n",
        "\n",
        "def get_all_labels(train_sents):\n",
        "  res=[]\n",
        "  for s in train_sents:\n",
        "    for k in s:\n",
        "      res.append(k[2])\n",
        "  diction=dict.fromkeys(res)\n",
        "  res1={ e for e in diction}\n",
        "  return res1\n",
        "\n",
        "print(sent2labels(train_sents[0]))\n",
        "print(sent2tokens(train_sents[0]))\n",
        "print(get_all_labels(train_sents))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n",
            "['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.']\n",
            "{'I-ORG', 'I-PER', 'I-MISC', 'B-MISC', 'B-LOC', 'B-ORG', 'B-PER', 'O', 'I-LOC'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg4KrX2vWoYi",
        "colab_type": "text"
      },
      "source": [
        "Oczekiwany rezultat:\n",
        "<pre>\n",
        "['B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n",
        "['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.']\n",
        "{'I-PER', 'I-MISC', 'B-LOC', 'I-LOC', 'B-PER', 'B-MISC', 'I-ORG', 'B-ORG', 'O'}\n",
        "</pre>\n",
        "\n",
        "Uruchom poniższy kod i sprawdź czego nauczył się nasz NER."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvosM5ToWoYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "4f64f82d-b147-48c3-e378-a273df53559e"
      },
      "source": [
        "X_train = [sent2features(s) for s in train_sents] # Stwórz cechy zbioru treningowego\n",
        "y_train = [sent2labels(s) for s in train_sents]   # Pobierz etykiety zbioru treningowego\n",
        "\n",
        "X_test = [sent2features(s) for s in test_sents]   # Stwórz cechy zbioru testowego\n",
        "y_test = [sent2labels(s) for s in test_sents]     # Pobierz etykiety zbioru testowego\n",
        "\n",
        "trainer = pycrfsuite.Trainer(verbose=False)    # stwórz obiekt trenujący\n",
        "\n",
        "for xseq, yseq in zip(X_train, y_train):       # iteruj po zdaniach i etykietach\n",
        "    trainer.append(xseq, yseq)                 # dopisuj do obiektu trenującego nasze dane\n",
        "    \n",
        "trainer.set_params({\n",
        "    'c1': 1.0,   # parametr regularyzacyjny L1\n",
        "    'c2': 1e-3,  # parametr regularyzacyjny L2\n",
        "    'max_iterations': 50,  # maksymalna liczba iteracji\n",
        "    # dodaj tranzycje, które nie są obserwowane ale są możliwe\n",
        "    'feature.possible_transitions': True\n",
        "})\n",
        "\n",
        "trainer.train('conll2002-esp.crfsuite')       # wytrenuj model i zapisz do pliku!\n",
        "\n",
        "tagger = pycrfsuite.Tagger()                  # stwórz tagger, który będzie nadawał etykiety naszej sekwencji\n",
        "tagger.open('conll2002-esp.crfsuite')         # załaduj do niego wytrenowany model\n",
        "example_sent = test_sents[0]                  # weź pierwsze z brzegu zdanie, które nie brało udziału w treningu\n",
        "print(' '.join(sent2tokens(example_sent)), end='\\n\\n')   # wyświetl je...\n",
        "\n",
        "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))  # zobacz, co generuje nasz model\n",
        "print(\"Correct:  \", ' '.join(sent2labels(example_sent)))                # i to, czego oczekiwano!"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La Coruña , 23 may ( EFECOM ) .\n",
            "\n",
            "Predicted: B-LOC I-LOC O O O O B-ORG O O\n",
            "Correct:   B-LOC I-LOC O O O O B-ORG O O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "pdSeyhHZWoYp",
        "colab_type": "text"
      },
      "source": [
        "## Ekstrakcja fraz rzeczownikowych\n",
        "\n",
        "Czasami chcielibyśmy z danego tekstu wyekstrahować nie tylko encje, na których nasz NER jest wytrenowany (pewien podzbiór kategorii), ale wszystkie frazy opisujące obiekty. Po co? przydać się to może np. przy tworzeniu tzw. chmury słów kluczowych, której przykład znajdziecie poniżej, bądź w problemach automatycznego odpowiadania na pytania.\n",
        "\n",
        "<img src=\"./cloud.jpg\"/>\n",
        "\n",
        "Wydawałoby się, że aby tego dokonać, sensownym podejściem byłoby zidentyfikowanie wszystkich rzeczowników, np. w zdaniu \"Ala ma piękny mały dom\", rzeczownikami są: \"Ala\", \"dom\". \n",
        "Ograniczając się do rzeczowników, tracimy jednak ważne informacje, które opisują rzeczowniki, np. bardzo istotne może być zapamiętanie, że dom Ali jest piękny i mały.\n",
        "\n",
        "Czy istnieje sposób automatycznego ekstrahowania całych tzw. fraz rzeczownikowych? (https://pl.wikipedia.org/wiki/Fraza_nominalna)\n",
        "\n",
        "Oczywiście, z pomocą przychodzą narzędzia takie jak SpaCy czy NLTK.\n",
        "\n",
        "**Zadanie4 (1 punkt)** Korzystając z dokumentacji SpaCy, zidentyfikuj wszystkie frazy rzeczownikowe (noun chunks) z zadanego zdania."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q75aov9WoYq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "267e6d9f-7ef5-435f-ed20-a159dff2e63e"
      },
      "source": [
        "import spacy\n",
        "sentence = \"Jim Gates bought 300 shares of Acme Corp and a tiny beautiful house.\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(sentence)\n",
        "for chunk in doc.noun_chunks:\n",
        "  print(chunk.text)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jim Gates\n",
            "300 shares\n",
            "Acme Corp\n",
            "a tiny beautiful house\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "veUSoCTDWoYw",
        "colab_type": "text"
      },
      "source": [
        "Oczekiwany rezultat:\n",
        "\n",
        "<pre>\n",
        "Jim Gates\n",
        "300 shares\n",
        "Acme Corp\n",
        "a tiny beautiful house\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxHgKqXrWoYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}